Embeddings are created by training neural network models to learn continuous vector representations of words or other language units, such as subwords or characters. These vector representations, or embeddings, capture the semantic and syntactic properties of the language units. There are several popular techniques to create embeddings, including Word2Vec, GloVe, FastText, and the transformer-based models like BERT and GPT. Here's a brief overview of how some of these methods work:

Word2Vec: This approach, developed by Tomas Mikolov and his team at Google, consists of two main architectures: Continuous Bag-of-Words (CBOW) and Skip-gram. CBOW predicts a target word based on its surrounding context words, while Skip-gram predicts context words given the target word. The model is trained on large amounts of text data using a shallow neural network. As a result of the training, each word is assigned a dense vector representation that captures its semantic and syntactic properties.

GloVe (Global Vectors for Word Representation): Developed by researchers at Stanford University, GloVe is based on the idea of using global co-occurrence statistics of words from a large corpus. The model learns embeddings by optimizing a weighted least-squares objective function that relates the logarithm of word co-occurrence probabilities to their vector dot products. This allows GloVe to capture both local and global information in the embeddings.

FastText: Developed by Facebook AI Research, FastText is an extension of the Word2Vec model. Instead of using whole words, FastText represents each word as an n-gram of characters (subwords). This allows the model to capture morphological information and create embeddings even for rare and out-of-vocabulary words. Like Word2Vec, FastText can use either the CBOW or Skip-gram architecture for training.

Transformer-based models (BERT, GPT, etc.): These models, based on the transformer architecture, learn contextualized embeddings by processing input text through multiple self-attention layers. BERT (Bidirectional Encoder Representations from Transformers) learns embeddings by training on masked language modeling and next sentence prediction tasks. GPT (Generative Pre-trained Transformer) learns embeddings by training on a unidirectional language modeling task. Both models are pre-trained on large amounts of text data and can be fine-tuned for specific downstream tasks.

The process of creating embeddings involves initializing the model with random weights, training it on a large text corpus, and adjusting the weights through optimization algorithms like stochastic gradient descent. Once the model is trained, the embeddings can be extracted and used for various natural language processing tasks, such as sentiment analysis, machine translation, or text classification.